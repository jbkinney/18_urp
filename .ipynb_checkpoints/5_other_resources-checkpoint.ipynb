{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: Other python resources\n",
    "\n",
    "### by Justin B. Kinney"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's tutorial will focus on useful python resources, including **pyfasta, biopython, seaborn, scikit-learn, and scipy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always put this first\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run today's code, you will need to install three packages: pyfasta, biopython, and seaborn. Do this at the shell (not python) command line:\n",
    "```\n",
    "$ conda install seaborn\n",
    "$ conda install biopython\n",
    "$ pip install pyfasta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple hacks for working with DNA and RNA sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some simple functions work working with DNA\n",
    "import string\n",
    "def reverse_complement(seq):\n",
    "    complement = str.maketrans('ATCGN', 'TAGCN')\n",
    "    return str(seq).upper().translate(complement)[::-1]\n",
    "\n",
    "def transcribe(seq):\n",
    "    return str(seq).upper().replace('T','U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = 'ACCCTCCATTACCCTGCCTCCACTCGTTACCCTGTCCCATTCAACCATACCACTCCGAAC'\n",
    "\n",
    "print('DNA:      ', seq)\n",
    "print('DNA (rc): ', reverse_complement(seq))\n",
    "print('RNA:      ', transcribe(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyfasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyfasta (https://pypi.python.org/pypi/pyfasta) provides a simple way to read sequences out of very large DNA sequence files in FASTA format. pyfasta can be installed with\n",
    "\n",
    "```\n",
    "$ pip install pyfasta\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview fasta file\n",
    "!head s288c.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a fasta file in a simple, pythonic manner\n",
    "import pyfasta\n",
    "my_fasta = pyfasta.Fasta('s288c.fa')\n",
    "\n",
    "# We interface with the FASTA file like we interface with dictionaries\n",
    "keys = list(my_fasta.keys())\n",
    "print('\\n'.join(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = keys[0]\n",
    "print(key, ':')\n",
    "print('Length = %d'%len(my_fasta[key]))\n",
    "print('Slice: %s'%my_fasta[key][120:180])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biopython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biopython provides routines for dealing with DNA sequence and performing other bioinformatics computations. The official website is http://biopython.org/wiki/Biopython. Biopython can be insalled by doing this at the command line:\n",
    "\n",
    "```\n",
    "$ conda install biopython\n",
    "```\n",
    "Personally, I find Biopython to be overly complicated. I actually only use it when translating DNA or RNA to protein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DNA sequence\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Alphabet import generic_dna\n",
    "\n",
    "# Create a DNA sequence\n",
    "my_dna = Seq(\"ATGGCCATTGTAATGGGCCGCTGAAAGGGTGCCCGATAG\", generic_dna)\n",
    "my_dna_rc = my_dna.reverse_complement()\n",
    "my_rna = my_dna.transcribe()\n",
    "\n",
    "print('DNA:', my_dna)\n",
    "print('DNA (rc):', my_dna_rc)\n",
    "print('RNA:', my_rna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate protein\n",
    "bad_protein = my_rna.translate()\n",
    "bad_protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate using a different codon table\n",
    "good_protein = my_rna.translate(table=\"Vertebrate Mitochondrial\")\n",
    "good_protein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is a package for working with columnar data. It comes standard with the anaconda installation of python. Pandas is incredibly useful, but there is a substantial learning curve. If you want to learn more about Pandas, \n",
    "here is a useful site: http://pandas.pydata.org/, and\n",
    "here is a book written by Wes McKinney (the author of pandas): http://shop.oreilly.com/product/0636920023784.do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the data parsing exercise now with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!open binding_site_db.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = 'binding_site_db.txt'\n",
    "column_indices = [1,11]\n",
    "column_names = ['name','site']\n",
    "dtype_dict = {'name':str, 'site':str}\n",
    "tf_df = pd.read_csv(in_file,sep='\\t',comment='#',usecols=column_indices,names=column_names,dtype=dtype_dict)\n",
    "tf_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaNs\n",
    "ok_indices = [type(site) is str for site in tf_df['site']]\n",
    "tf_df = tf_df[ok_indices]\n",
    "\n",
    "# Show df\n",
    "tf_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caplitalize sites\n",
    "tf_df['site'] = [site.upper() for site in tf_df['site']]\n",
    "\n",
    "# Comptue site lengths\n",
    "tf_df['length'] = [len(site) for site in tf_df['site']]\n",
    "\n",
    "tf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract rows for TF of interest\n",
    "df = tf_df[tf_df['name']=='CRP'].copy()\n",
    "\n",
    "# Only take rows with sites of the right size\n",
    "length_mode = df['length'].mode()[0]\n",
    "df = df[df['length']==length_mode]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn this into a function that can process any TF name!\n",
    "def get_counts_matrix(tf_name):\n",
    "    \n",
    "    # Make sure TF is in database\n",
    "    tf_names = set(tf_df['name'])\n",
    "    assert tf_name in tf_names, 'Error! No TF named %s in database.'%tf_name\n",
    "    \n",
    "    # Extract rows for TF of interest\n",
    "    df = tf_df[tf_df['name']==tf_name]\n",
    "\n",
    "    # Only take rows with sites of the right size\n",
    "    length_mode = df['length'].mode()[0]\n",
    "    df = df[df['length']==length_mode]\n",
    "\n",
    "    # Fill counts matrix\n",
    "    counts_matrix = np.zeros([length_mode,4])\n",
    "    bases = 'ACGT'\n",
    "    for s in df['site']:\n",
    "        for i in range(length_mode):\n",
    "            for b, base in enumerate(bases):\n",
    "                counts_matrix[i,b] += (s[i] == base)\n",
    "                \n",
    "    return counts_matrix\n",
    "\n",
    "# Test function\n",
    "get_counts_matrix('CRP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib is the standard plotting utility in python. I use it all the time to make publication quality plots. The main matplotlib page is here: https://matplotlib.org/index.html. A gallery of things you can do is here: https://matplotlib.org/gallery.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seaborn module, described here, https://stanford.edu/~mwaskom/software/seaborn/, provides very convenient plotting functionality. The plotting functions in seaborn do a lot more than those in matplotlib (ontop of which they are built). Seaborn produces matplotlib objects, and all matplotlib functions can be used in conjunction with seaborn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: Seaborn messes with the default styles, so once you load it, expect plots produced by matplotlib to look different. This behavior can be controlled with the 'sns.set()' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the convention for importing seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "\n",
    "counts_mat = get_counts_matrix('CRP')\n",
    "plt.imshow(counts_mat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, axes = plt.subplots(2, 2, figsize=(7, 7), sharex=True)\n",
    "sns.despine(left=True)\n",
    "\n",
    "# Generate a random univariate dataset\n",
    "d = np.random.randn(100)\n",
    "\n",
    "# Plot a simple histogram with binsize determined automatically\n",
    "sns.distplot(d, kde=False, color=\"b\", ax=axes[0, 0])\n",
    "\n",
    "# Plot a kernel density estimate and rug plot\n",
    "sns.distplot(d, hist=False, rug=True, color=\"r\", ax=axes[0, 1])\n",
    "\n",
    "# Plot a filled kernel density estimate\n",
    "sns.distplot(d, hist=False, color=\"g\", kde_kws={\"shade\": True}, ax=axes[1, 0])\n",
    "\n",
    "# Plot a historgram and kernel density estimate\n",
    "sns.distplot(d, color=\"m\", ax=axes[1, 1])\n",
    "\n",
    "plt.setp(axes, yticks=[])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn comes with some standard data sets\n",
    "\n",
    "# Load dataset\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint plot\n",
    "sns.set(style=\"darkgrid\", color_codes=True)\n",
    "\n",
    "g = sns.jointplot(\"total_bill\", \"tip\", data=tips, kind=\"reg\",\n",
    "                  xlim=(0, 60), ylim=(0, 12), color=\"r\", size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "rs = np.random.RandomState(11)\n",
    "x = rs.gamma(2, size=1000)\n",
    "y = -.5 * x + rs.normal(size=1000)\n",
    "\n",
    "sns.jointplot(x, y, kind=\"hex\", stat_func=kendalltau, color=\"#4CB391\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plot\n",
    "sns.set()\n",
    "\n",
    "# Create a random dataset across several variables\n",
    "rs = np.random.RandomState(0)\n",
    "n, p = 40, 8\n",
    "d = rs.normal(0, 2, (n, p))\n",
    "d += np.log(np.arange(1, p + 1)) * -5 + 10\n",
    "\n",
    "# Use cubehelix to get a custom sequential palette\n",
    "pal = sns.cubehelix_palette(p, rot=+.5, dark=.3)\n",
    "\n",
    "# Show each distribution with both violins and points\n",
    "sns.violinplot(data=d, palette=pal, inner=\"points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Load the long-form example gammas dataset\n",
    "gammas = sns.load_dataset(\"gammas\")\n",
    "\n",
    "# Plot the response with standard error\n",
    "sns.tsplot(data=gammas, time=\"timepoint\", unit=\"subject\",\n",
    "           condition=\"ROI\", value=\"BOLD signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "sns.set()\n",
    "\n",
    "# Load the example flights dataset and conver to long-form\n",
    "flights_long = sns.load_dataset(\"flights\")\n",
    "flights = flights_long.pivot(\"month\", \"year\", \"passengers\")\n",
    "\n",
    "# Draw a heatmap with the numeric values in each cell\n",
    "sns.heatmap(flights, annot=True, fmt=\"d\", linewidths=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn is the premeir machine learning module for python. The package and documentation can be found here: http://scikit-learn.org/stable/index.html. This module comes pre-installed in the Anaconda distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of handwritten numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================\n",
    "Recognizing hand-written digits\n",
    "================================\n",
    "\n",
    "An example showing how the scikit-learn can be used to recognize images of\n",
    "hand-written digits.\n",
    "\n",
    "This example is commented in the\n",
    ":ref:`tutorial section of the user manual <introduction>`.\n",
    "\n",
    "\"\"\"\n",
    "print(__doc__)\n",
    "\n",
    "# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "print(digits.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print one of the images\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "image = digits['images'][0]\n",
    "target = digits['target'][0]\n",
    "plt.imshow(digits['images'][0], interpolation='nearest')\n",
    "print('target = ', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data that we are interested in is made of 8x8 images of digits, let's\n",
    "# have a look at the first 3 images, stored in the `images` attribute of the\n",
    "# dataset.  If we were working from image files, we could load them using\n",
    "# pylab.imread.  Note that each image must have the same size. For these\n",
    "# images, we know which digit they represent: it is given in the 'target' of\n",
    "# the dataset.\n",
    "plt.figure(figsize=[15,15])\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:40]):\n",
    "    plt.subplot(5, 8, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How large is the training data? \n",
    "n_samples = len(digits.images)\n",
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "classifier = svm.SVC(gamma=0.001)\n",
    "\n",
    "#### This is where we fit the classifier: ###\n",
    "# We learn the digits on the first half of the digits\n",
    "classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now predict the value of the digit on the second half:\n",
    "expected = digits.target[n_samples // 2:]\n",
    "predicted = classifier.predict(data[n_samples // 2:])\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\n",
    "for index, (image, prediction) in enumerate(images_and_predictions[:4]):\n",
    "    plt.subplot(2, 4, index + 5)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Prediction: %i' % prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=====================\n",
    "Classifier comparison\n",
    "=====================\n",
    "\n",
    "A comparison of a several classifiers in scikit-learn on synthetic datasets.\n",
    "The point of this example is to illustrate the nature of decision boundaries\n",
    "of different classifiers.\n",
    "This should be taken with a grain of salt, as the intuition conveyed by\n",
    "these examples does not necessarily carry over to real datasets.\n",
    "\n",
    "Particularly in high-dimensional spaces, data can more easily be separated\n",
    "linearly and the simplicity of classifiers such as naive Bayes and linear SVMs\n",
    "might lead to better generalization than is achieved by other classifiers.\n",
    "\n",
    "The plots show training points in solid colors and testing points\n",
    "semi-transparent. The lower right shows the classification accuracy on the test\n",
    "set.\n",
    "\"\"\"\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree\",\n",
    "         \"Random Forest\", \"AdaBoost\", \"Naive Bayes\", \"Linear Discriminant Analysis\",\n",
    "         \"Quadratic Discriminant Analysis\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds in datasets:\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "    # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot also the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "figure.subplots_adjust(left=.02, right=.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scipy (https://www.scipy.org/) is the primary scientific python toolkit. There is lots of useful math stuff in here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very useful thing is the large set of statistical functions provided by scipy. Here is a list: https://docs.scipy.org/doc/scipy/reference/stats.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Define distribution\n",
    "norm = stats.norm()\n",
    "\n",
    "# Draw random numbers from a normal distribution\n",
    "xs = norm.rvs(size=1000)\n",
    "sns.distplot(xs)\n",
    "\n",
    "# Draw the true distribution\n",
    "x_grid = np.linspace(-4,4,100)\n",
    "plt.plot(x_grid,norm.pdf(x_grid),'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define distribution\n",
    "shape_param = 2.0\n",
    "gamma = stats.gamma(shape_param)\n",
    "\n",
    "# Draw random numbers from a normal distribution\n",
    "xs = gamma.rvs(size=1000)\n",
    "sns.distplot(xs)\n",
    "\n",
    "# Draw the true distribution\n",
    "x_grid = np.linspace(0,10,100)\n",
    "plt.plot(x_grid, gamma.pdf(x_grid),'k')\n",
    "\n",
    "plt.title('Gamma distribution, shape = %d'%shape_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ I(n,x) = \\int_1^\\infty dt \\frac{e^{-xt}}{t^n} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "\n",
    "# Numerical ntegration \n",
    "def integrand(t, n, x):\n",
    "    return np.exp(-x*t) / t**n\n",
    "n = 5\n",
    "x = 4\n",
    "I = integrate.quad(integrand, 1, np.inf, args=(n, x))\n",
    "print('integral value: I = %f'%I[0])\n",
    "print('absolute error <= %e'%I[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to solve the second-order Oridnary Differential Equation (ODE),\n",
    "\n",
    "$$ \\theta''(t) + b*\\theta'(t) + c*\\sin(\\theta(t)) = 0 $$\n",
    "\n",
    "We do this by changing to a systems of multiple first-order ODEs:\n",
    "\n",
    "$$ \\theta'(t) = \\omega(t) $$\n",
    "$$ \\omega'(t) = -b*\\omega(t) - c*\\sin(\\theta(t)) $$\n",
    "\n",
    "Systems of first-order ODEs like this can readilty be solved using scipy.integrate.odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import odeint\n",
    "\n",
    "def pend(y, t, b, c):\n",
    "    theta, omega = y\n",
    "    dydt = [omega, -b*omega - c*np.sin(theta)]\n",
    "    return dydt\n",
    "\n",
    "b = 0.25\n",
    "c = 5.0\n",
    "\n",
    "y0 = [np.pi - 0.1, 0.0]\n",
    "\n",
    "t = np.linspace(0, 10, 101)\n",
    "sol = odeint(pend, y0, t, args=(b, c))\n",
    "\n",
    "plt.plot(t, sol[:, 0], 'b', label=r'$\\theta$(t)')\n",
    "plt.plot(t, sol[:, 1], 'g', label=r'$\\omega$(t)')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('t')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda_kernel",
   "language": "python",
   "name": "anaconda_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
